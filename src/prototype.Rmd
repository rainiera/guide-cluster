---
title: "guide_clustering_prototype"
author: "rainier"
date: "March 28, 2016"
output: html_document
---

```{r global_options, include=FALSE}
library(knitr)
library(ggplot2)
theme_set(theme_bw(base_size=12))
library(dplyr)
```

Hacky prototyping because I like dplyr.
In no way reflects my Python style as I have yet to find a PEP equivalent in R.


**Exploratory analysis - PCA on numeric data** (no feature creation/vectorizing yet)  


I want to see which features contribute to principal components that account for most of the variance in the dataset.

_Steps_  
1) Perform PCA and examine the rotation matrix.  
2) Determine how much each PC contributes to the variance.
```{r}
link_to_data <- "https://github.com/rainiera/guide-cluster/blob/master/data/example_guide_data.tsv?raw=true"
guide_data = read.table(link_to_data, sep="\t", header=TRUE)

# Here's what the data looks like
head(guide_data)

# Select the non-numeric features, perform PCA, and look at the rotation matrix
guide_data %>%
    select(-c(gene_name, spacer_id, spacer_seq)) %>%
    scale() %>%
    prcomp() ->
    pca_data
pca_data$rotation

# Create and plot the rotation matrix to visualize which features contribute to the principal components
rotation_data <- data.frame(pca_data$rotation, variable=row.names(pca_data$rotation))
arrow_style <- arrow(length = unit(0.05, "inches"),
                     type = "closed")
ggplot(rotation_data) + 
    geom_segment(aes(xend=PC1, yend=PC2), x=0, y=0, arrow=arrow_style) +
    geom_text(aes(x=PC1, y=PC2, label=variable), hjust=0, size=3, color='red') +
    xlim(-0.0, 0.5) +
    ylim(-0.2, 0.3) +
    coord_fixed()

# Make a bar graph to see how much % each component explains the variance
percent <- 100*pca_data$sdev^2/sum(pca_data$sdev^2)
perc_data <- data.frame(percent=percent, PC=1:length(percent))
ggplot(perc_data, aes(x=PC, y=percent)) +
    geom_bar(stat="identity") +
    geom_text(aes(label=round(percent,2)), size=4, vjust=-.5) +
    ylim(0,80)

```


Counts from the PLX treatment group seem to contribute to an entirely different principal component.



**Adding activities to spacer sequences**


PLX seems to be a drug for potentially treating malignant melanoma (probably *these* cancer cell lines). ([Smalley 2010](http://www.ncbi.nlm.nih.gov/pubmed/20496265).)

Moreover, the rotation matrix from the PCA suggested that counts from the PLX treatment group contributed to an entirely different principal component. Thusly, I will leave it out for now and focus on the normalized counts without drug treatment.

_Steps_  
1) Average replicates for counts from 1 week and 2 weeks since plasmid introduction and into new columns, `norm_count_D7_mean` and `norm_count_D14_mean`.  
2) Compute guide activities between days 0-7, 7-14, 0-14 and place in new columns, `act_0_7`, `act_7_14`, `act_0_14`.
```{r}

# Average replicates, leave only needed features
guide_data %>%
    mutate(norm_count_D7_mean = (norm_count_D7_Rep1 + norm_count_D7_Rep2) / 2) %>%
    mutate(norm_count_D14_mean = (norm_count_D14_Rep1 + norm_count_D14_Rep2) / 2) %>%
    select(c(gene_name,
             spacer_id,
             spacer_seq,
             norm_count_plasmid,
             norm_count_D7_mean,
             norm_count_D14_mean))-> guide_data_modified

# Compute guide activities
guide_data_modified %>%
    mutate(act_0_7 = -log2((norm_count_D7_mean/norm_count_plasmid))) %>%
    mutate(act_7_14 = -log2((norm_count_D14_mean/norm_count_D7_mean))) %>%
    mutate(act_0_14 = -log2((norm_count_D14_mean/norm_count_plasmid))) -> guide_data_acts

# Sanity check
head(guide_data_acts)
```


**Vectorize spacer sequence nucleotide frequencies [WIP]**


I want to count nucleotide frequencies in each spacer sequencies and vectorize them. Akin to using a Counter in Python and applying scikit-learn DictVectorizer.

```{r}
```


**K-means**


I want to cluster the activities.

_Steps_
1) Plot within-groups sum of squares against the number of clusters to find the slowdown at k number of clusters. [Stack Overflow post](http://stackoverflow.com/questions/15376075/cluster-analysis-in-r-determine-the-optimal-number-of-clusters/15376462#15376462)
2) Perform k-means.

```{r}
# within groups sum of squares

set.seed(1234)

guide_data_acts %>%
    select(c(act_0_7, act_7_14, act_0_14)) -> guide_acts_numeric

wgss <- (nrow(guide_data_acts)-1) * sum(apply(guide_acts_numeric,2,var))
for (i in 2:15) wgss[i] <- sum(kmeans(guide_acts_numeric,
                                      nstart=10,
                                      centers=i)$withinss)
wgss_data <- data.frame(centers=1:15, wgss)
ggplot(wgss_data, aes(x=centers, y=wgss)) +
    geom_point() +
    geom_line() +
    xlab("Number of clusters") + ylab("Within groups sum of squares")

guide_acts_numeric %>%
    kmeans(centers=3) -> km

guide_acts_cluster <- data.frame(guide_acts_numeric, cluster=factor(km$cluster))

# for fast debugging - take random subsample of cluster data
cluster_sample <- guide_acts_cluster[sample(1:nrow(guide_acts_cluster), 1000, replace=FALSE),]

ggplot(cluster_sample, aes(x=act_7_14, y=act_0_14, color=cluster)) + geom_point()

```
